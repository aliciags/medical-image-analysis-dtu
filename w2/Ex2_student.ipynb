{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3c2d558",
   "metadata": {},
   "source": [
    "# Landmark-based Registration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfeae45-c6f1-48f7-8df8-34851c8d9acc",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this exercise, you will implement 3D rigid and affine image registration based on anatomical landmarks. You will work with a T1-weighted and a T2-weighted MRI scan of the same patient, where the patient is lying in a different position in the scanner between the two image acquisition (for the purpose of this exercise, the movement is simulated).\n",
    "\n",
    "### Instructions for your report\n",
    "Your report should be structured as follows:\n",
    "* **Introduction**: a short summary of what your report is about, perhaps with a recap of some of the equations you'll be using in your solution.\n",
    "* **Task 1, ..., 5**: for each specific task listed below, your code (in code cells), your results (as figures) as well as explanations of what is computed and what is shown in the figures (in Markdown cells). \n",
    "* **Conclusion**: a short summary of your findings.\n",
    "\n",
    "This introduction should **not** be part of your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4f12a8f-7edb-4f65-9a7e-58b0b2ec76d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipython in /opt/anaconda3/lib/python3.12/site-packages (8.25.0)\n",
      "Requirement already satisfied: decorator in /opt/anaconda3/lib/python3.12/site-packages (from ipython) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/anaconda3/lib/python3.12/site-packages (from ipython) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/anaconda3/lib/python3.12/site-packages (from ipython) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/anaconda3/lib/python3.12/site-packages (from ipython) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from ipython) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /opt/anaconda3/lib/python3.12/site-packages (from ipython) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from ipython) (5.14.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/anaconda3/lib/python3.12/site-packages (from ipython) (4.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from jedi>=0.16->ipython) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/anaconda3/lib/python3.12/site-packages (from pexpect>4.3->ipython) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython) (0.2.5)\n",
      "Requirement already satisfied: executing in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython) (0.2.2)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.12/site-packages (from asttokens->stack-data->ipython) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88abf658-92bc-4c5b-adcb-3c43a02c451d",
   "metadata": {},
   "source": [
    "### Input data and code hints\n",
    "Import Python libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c00a4b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8.25.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib tk\n",
    "plt.ion()\n",
    "import numpy as np\n",
    "np.set_printoptions( suppress=True )\n",
    "import nibabel as nib\n",
    "import scipy\n",
    "import IPython\n",
    "IPython.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3273d999-6015-4e04-ba0f-103a683fb5bb",
   "metadata": {},
   "source": [
    "Read the two 3D scans you'll be working with in this exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aafeca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load T1 and T2 data\n",
    "T1_fileName = 'IXI014-HH-1236-T1.nii.gz'\n",
    "T2_fileName = 'IXI014-HH-1236-T2_moved.nii.gz'\n",
    "T1 = nib.load( T1_fileName )\n",
    "T2 = nib.load( T2_fileName )\n",
    "T1_data = T1.get_fdata()\n",
    "T2_data = T2.get_fdata()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303e356d-295a-456b-9984-baf0bc573ce6",
   "metadata": {},
   "source": [
    "Below is code to define a simple interactive viewer class that can be used to visualize 2D cross-sections of a 3D array along three orthogonal directions. It takes a 3D volume as input and shows the location a \"linked cursor\" in all three cross-sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e8646cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Viewer:\n",
    "    def __init__(self, data ):\n",
    "        self.fig, self.ax = plt.subplots()\n",
    "        self.data = data\n",
    "        self.dims = self.data.shape\n",
    "        self.position = np.round( np.array( self.dims ) / 2 ).astype( int )\n",
    "        self.draw()\n",
    "        self.fig.canvas.mpl_connect( 'button_press_event', self )\n",
    "        self.fig.show()\n",
    "\n",
    "    def __call__(self, event):\n",
    "        print( 'button pressed' )\n",
    "        if event.inaxes is None: return\n",
    "      \n",
    "        x, y = round( event.xdata ), round( event.ydata )\n",
    "\n",
    "        #\n",
    "        if ( x > (self.dims[0]-1) ) and ( y <= (self.dims[1]-1) ): return # lower-right quadrant\n",
    "          \n",
    "        #\n",
    "        if x < self.dims[0]:\n",
    "          self.position[ 0 ] = x\n",
    "        else:\n",
    "          self.position[ 1 ] = x - self.dims[0]\n",
    "        \n",
    "        if y < self.dims[1]:\n",
    "          self.position[ 1 ] = y\n",
    "        else:\n",
    "          self.position[ 2 ] = y -self.dims[1]\n",
    "        \n",
    "        print( f\"  voxel index: {self.position}\" )\n",
    "        print( f\"  intensity: {self.data[ self.position[0], self.position[1], self.position[2] ]}\" )\n",
    "\n",
    "        self.draw()\n",
    "\n",
    "    def draw( self ):\n",
    "        #\n",
    "        # Layout on screen is like this:\n",
    "        #\n",
    "        #     ^            ^\n",
    "        #  Z  |         Z  |\n",
    "        #     |            |\n",
    "        #     ----->        ---->  \n",
    "        #       X             Y\n",
    "        #     ^\n",
    "        #  Y  |\n",
    "        #     |\n",
    "        #     ----->  \n",
    "        #       X\n",
    "        #\n",
    "        dims = self.dims\n",
    "        position = self.position\n",
    "        \n",
    "        xySlice = self.data[ :, :, position[ 2 ] ]\n",
    "        xzSlice = self.data[ :, position[ 1 ], : ]\n",
    "        yzSlice = self.data[ position[ 0 ], :, : ]\n",
    "        \n",
    "        kwargs = dict( vmin=self.data.min(), vmax=self.data.max(), \n",
    "                       origin='lower', \n",
    "                       cmap='gray',\n",
    "                       picker=True )\n",
    "\n",
    "        self.ax.clear()\n",
    "\n",
    "        self.ax.imshow( xySlice.T, \n",
    "                        extent=( 0, dims[0]-1, \n",
    "                                 0, dims[1]-1 ), \n",
    "                        **kwargs )\n",
    "        self.ax.imshow( xzSlice.T, \n",
    "                        extent=( 0, dims[0]-1, \n",
    "                                 dims[1], dims[1]+dims[2]-1 ), \n",
    "                        **kwargs )\n",
    "        self.ax.imshow( yzSlice.T, extent=( dims[0], dims[0]+dims[1]-1, \n",
    "                                            dims[1], dims[1]+dims[2]-1 ), \n",
    "                        **kwargs )\n",
    "\n",
    "        color = 'g'\n",
    "        self.ax.plot( (0, dims[0]-1), (position[1], position[1]), color )\n",
    "        self.ax.plot( (0, dims[0]+dims[1]-1), (dims[1]+position[2], dims[1]+position[2]), color )\n",
    "        self.ax.plot( (position[0], position[0]), (0, dims[1]+dims[2]-1), color )\n",
    "        self.ax.plot( (dims[0]+position[1], dims[0]+position[1]), (dims[1]+1, dims[1]+dims[2]-1), color )\n",
    "\n",
    "        self.ax.set( xlim=(1, dims[0]+dims[1]), ylim=(0, dims[1]+dims[2]) )\n",
    "\n",
    "        self.ax.text( dims[0] + dims[1]/2, dims[1]/2, \n",
    "                      f\"voxel index: {position}\",  \n",
    "                      horizontalalignment='center', verticalalignment='center' )\n",
    "  \n",
    "        self.ax.axis( False )\n",
    "\n",
    "        self.fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccd2de5-8803-4f20-ad11-995425864206",
   "metadata": {},
   "source": [
    "The code below shows how to visualize the T1-weigthed and the T2-weighted volumes with this viewer class. The initial location of the cursor is in the middle of the volume in each case. It can be changed by clicking on one of the cross-sections. The viewer also displays the voxel index $\\mathbf{v}$ of the cursor. Play around and try to understand what the Viewer() class does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2045427-0c33-4b62-a0f2-adfa98fdffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "T1_viewer = Viewer( T1_data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a4b223d-0be2-43a6-bbcf-dfe08c6abd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "T2_viewer = Viewer( T2_data )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c262d189",
   "metadata": {},
   "source": [
    "### Task 1: Coordinate Systems\n",
    "\n",
    "Familiarize yourself with the concept of coordinate systems. In your report, explain why we need to differentiate between \"voxel coordinates/indices\" $\\mathbf{v}$ and \"world coordinates\" $\\mathbf{x}$. Why does the T2-weighted volume look so compressed in the viewer? For the enthusiastic student: calculate the voxel size in each dataset.\n",
    "\n",
    "The world coordinate system used in both the T1-weighted and the T2-weighted scan follows the RAS convention. Equipped with this information, determine the voxel index $\\mathbf{v}$ of the center of the left eye of the patient in the T1-weighted scan. Do the same for the T2-weighted scan.\n",
    " \n",
    "> ***Hints:***\n",
    "> \n",
    "> - The affine voxel-to-world matrix of the T1-weighted scan is given by\n",
    ">\n",
    ">        T1.affine\n",
    ">\n",
    "> \n",
    "> - In nibabel, the RAS convention is used (see bottom of https://nipy.org/nibabel/coordinate_systems.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2819fd67",
   "metadata": {},
   "source": [
    "The voxel coordinates/indices reffer to the 3 different numbers, usually i,j,k; they are the grid indices of individual volume elements. \n",
    "The world coordinates represents the actual topology of the patient. We would think this both would always the same ratio, but actually it doesnt. \n",
    "The MRI scan is not always isotropic, the spacing between voxels differs across the three dimensions. \n",
    "T2 looks compressed in the viewer because the spacing between voxels in the slice plane (e.g., x and y dimensions) is often much smaller than the slice thickness (z-dimension), resulting in the appearance of compression or stretching along the z-axis when visualized (it is an MRI scan). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "183eec30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voxel size of T1: [0.01506851 0.12376414 0.01646696] mm\n",
      "Voxel size of T2: [0.83986628 0.86080861 4.79742765] mm\n",
      "World coordinates of T1 for the left patient eye will be:  [32.75329093 75.34755456 -0.436423  ] mm\n",
      "World coordinates of T2 for the left patient eye will be:  [-19.56145275 116.24901803   3.76028705] mm\n"
     ]
    }
   ],
   "source": [
    "# Voxel size\n",
    "T1_affine = T1.affine\n",
    "# print (T1_affine)\n",
    "T2_affine = T2.affine\n",
    "\n",
    "voxel_size_T1= np.abs(np.diag(T1_affine)[:3]) \n",
    "print (f\"Voxel size of T1: {voxel_size_T1} mm\")\n",
    "\n",
    "voxel_size_T2= np.abs(np.diag(T2_affine)[:3])\n",
    "print (f\"Voxel size of T2: {voxel_size_T2} mm\")\n",
    "\n",
    "# Voxel index v of the center of the left eye of the patient \n",
    "#  x= Av +t where x is the world coordinate, v is the voxel index, A is the affine matrix and t is the translation vector\n",
    "#  A^-1(x -t) = v  where A^-1 is the inverse of the affine matrix\n",
    "\n",
    "A = np.linalg.inv(T1.affine[:3,:3])  \n",
    "t= T1_affine[:3,3]\n",
    "#print(t)\n",
    "voxel_indx =[64, 120, 103,1] #Left eye of the patient \n",
    "#print (voxel_indx)\n",
    "x = T1_affine @ voxel_indx\n",
    "print (\"World coordinates of T1 for the left patient eye will be: \",x[:3] ,\"mm\") \n",
    "\n",
    "A = np.linalg.inv(T2.affine[:3,:3])  \n",
    "t= T2_affine[:3,3]\n",
    "#print(t)\n",
    "voxel_indx =[141, 216, 15,1] #Left eye of the patient \n",
    "#print (voxel_indx)\n",
    "x = T2_affine @ voxel_indx\n",
    "print (\"World coordinates of T2 for the left patient eye will be: \",x[:3] ,\"mm\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc112c3",
   "metadata": {},
   "source": [
    "For this task it is very important to know the difference between world coordinates and voxel coordinates. Voxel coordinates refer to the position of a voxel in a 3D grid (so it has i, j, k) that makes up the MRI volume. World coordinates, on the other hand, represent the actual position of a voxel in the patientâ€™s body and are expressed in millimeters. To transform voxel coordinates to world coordinates we use x=Av+t as stated on the slides. The affine matrix has important information regarding scaling, rotation and translation. Being the diagonal of the matrix (except the element in position 4,4 which is a 1) the voxel size. We then got the world coordinates from the voxel coordinates that we measured in the lef eye by using the same formula but with the inverse of the affine matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4532ea",
   "metadata": {},
   "source": [
    "### Task 2: Resample the T2-weighted scan to the image grid of the T1-weighted scan\n",
    "\n",
    "In this task you should resample the T2-weighted scan to the image grid of the T1-weighted scan, i.e., create a new 3D volume that has the same size as the T1-weighted volume, but that contains interpolated T2-weighted intensities instead. In particular, for each voxel index $\\mathbf{v}_{T1}$ in the T1-weighted image grid, you should compute the corresponding voxel index $\\mathbf{v}_{T2}$ in the T2-weighted volume as follows (see section 2.1 in the book):\n",
    "$$\n",
    "\\begin{pmatrix} \\mathbf{ v_{T2}} \\\\ 1 \\end{pmatrix} = \\mathbf{M}_{T2}^{-1} \\cdot \\mathbf{M}_{T1} \\cdot \\begin{pmatrix} \\mathbf{ v_{T1}} \\\\ 1 \\end{pmatrix}\n",
    ".\n",
    "$$\n",
    "At the location $\\mathbf{v}_{T2}$, you should then use cubic B-spline interpolation to determine the intensity in the T2-weighted scan, and store it at index $\\mathbf{v}_{T1}$ in the newly created image.\n",
    "\n",
    "Once you have created a new volume like this, visualize it overlaid on the T1-weighted volume as follows:\n",
    "    \n",
    "    Viewer( T2_data_resampled / T2_data_resampled.max() + T1_data / T1_data.max() )\n",
    "\n",
    "> ***Hints:***\n",
    "> - you can create a coordinate grid in 3D with the function\n",
    "> \n",
    ">        V1,V2,V3 = np.meshgrid( np.arange( T1_data.shape[0] ), \n",
    ">                                np.arange( T1_data.shape[1] ), \n",
    ">                                np.arange( T1_data.shape[2] ), indexing='ij' )\n",
    ">   \n",
    ">\n",
    "> - the following SciPy function interpolates the T2-weighted volume at voxel coordinates $(1.1,2.2,3.3)^T$ \n",
    "> and $(6.6,7.7,8.8)^T$ using cubic interpolation:   \n",
    ">\n",
    ">        scipy.ndimage.map_coordinates( T2_data, np.array( [ [1.1,2.2,3.3], [6.6,7.7,8.8] ] ).T )\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d418bcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "V1,V2,V3 = np.meshgrid( np.arange( T1_data.shape[0] ), \n",
    "                         np.arange( T1_data.shape[1] ), \n",
    "                         np.arange( T1_data.shape[2] ), indexing='ij' )\n",
    "T1_voxel_coords = np.vstack([V1.ravel(), V2.ravel(), V3.ravel(), np.ones(V1.size)])\n",
    "\n",
    "T2_voxel_coords_n = np.linalg.inv(T2_affine) @ T1_affine @T1_voxel_coords\n",
    "T2_voxel_coords = T2_voxel_coords_n[:3,:]\n",
    "\n",
    "T2_data_resampled = scipy.ndimage.map_coordinates(T2_data, T2_voxel_coords, order=3).reshape(T1_data.shape)\n",
    "T2_viewer_r = Viewer( T2_data_resampled / T2_data_resampled.max() + T1_data / T1_data.max() )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99004f98",
   "metadata": {},
   "source": [
    "In summary we used MT1 and MT2, the affine matrices for the T1 and T2 scans and used the formula given to find the location in the T2 scan for each voxel in the T1 scan.\n",
    "We first created the meshgrid that represent the voxel indices in T1. We  then stack them  into a matrix after using ravel which transforms a 3D array into a 1D array and we also addeed an extra row on 1. We the create a matrix where each column has a 3D voxel coordinate in T2 and we take the first 3 rows to have te actual T2 voxel coordinates without the ones. Then we did a cubib B spline interpolation of order 3 to know the values of the voxels that are not integers and then reshaped into T1 dimensions.\n",
    "Finally we normalized by dividing by their max value so we get intensities that are between 0 and 1 and overlaid the two volumes so we can actually see the T2-weitghted scan onto the T1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4165b671",
   "metadata": {},
   "source": [
    "### Task 3: Collect corresponding landmarks\n",
    "\n",
    "Using the viewer class, record the voxel coordinates $\\mathbf{v_{T1}}$ and $\\mathbf{v_{T2}}$ of at least five corresponding landmarks in the T1- and the T2-weighted volumes, respectively. List them in your report, and explain why you picked them. \n",
    "\n",
    "> ***Hint:***\n",
    "> - Avoid picking landmarks that are very close to each other or that all lie approximately in the same 2D plane.\n",
    "> - You can double-check which landmarks you've selected as follows:\n",
    ">         T1_viewer = Viewer( T1_data )\n",
    ">         T1_viewer.position = ( 20, 30, 40 )\n",
    ">         T1_viewer.draw()\n",
    "> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed33eda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t1\n",
    "nose = [20, 85, 75]\n",
    "nuca_alta = [233, 124, 52]\n",
    "nuca_baja = [187, 66, 106]\n",
    "barbilla = [55, 10, 82]\n",
    "frente = [52, 166, 86]\n",
    "\n",
    "# t2\n",
    "nose = [120, 221, 8]\n",
    "nuca_alta = [132, 35, 22]\n",
    "nuca_baja = [132, 34, 1]\n",
    "barbilla = [126, 223, 2]\n",
    "frente = [121, 204, 23]\n",
    "\n",
    "#Another set of landmarks: \n",
    "# t1\n",
    "ojo_dcho = [61, 114, 107]\n",
    "nuca_alta = [218, 163, 46]\n",
    "nuca_baja = [210, 52, 65]\n",
    "oreja_dcha = [153, 96, 21]\n",
    "frente = [51, 174, 78]\n",
    "\n",
    "# t2\n",
    "ojo_dcho =[18, 114, 106]\n",
    "nuca_alta = [185, 147, 95]\n",
    "nuca_baja = [172, 23, 109]\n",
    "oreja_dcha = [126, 223, 2]\n",
    "frente = [86, 83, 35]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed076602",
   "metadata": {},
   "source": [
    "For this part we were just had to select landmarks using the viewer taking into acount that they had to be evenly distributed , anatomically recognizable and in different planes to provide a good 3D view."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fed8836",
   "metadata": {},
   "source": [
    "### Task 4: Perform affine landmark-based registration \n",
    "\n",
    "Using the landmarks you recorded in the previous task, compute the parameters of the 3D affine transformation that brings the landmarks in the T1-weighted image closest to the corresponding ones in the T2-weighted image. For this purpose, use Equation (2.8) in the book.\n",
    "\n",
    "Once you've determined the affine transformation, register to two images by resampling the T2-weighted image to the image grid of the T1-weighted image, and overlay the two images as in Task 2. To map voxel coordinates $\\mathbf{v}_T1$ to $\\mathbf{v}_T2$, you'll have to use \n",
    "$$\n",
    "\\begin{pmatrix} \\mathbf{ v_{T2}} \\\\ 1 \\end{pmatrix} = \\mathbf{M}_{T2}^{-1} \\cdot \\mathbf{M} \\cdot \\mathbf{M}_{T1} \\cdot \\begin{pmatrix} \\mathbf{ v_{T1}} \\\\ 1 \\end{pmatrix}\n",
    ",\n",
    "$$\n",
    "where $\\mathbf{M}$ is your $4 \\times 4$ affine matrix (see book).\n",
    "\n",
    "What happens when you increase/decrease the number of corresponding landmarks that are used in the computations? Comment.\n",
    "\n",
    "> ***Hint:***\n",
    "> - remember that the affine matrix works in *world* coordinates, so you'll have to map your landmarks to world coordinates first.\n",
    "> - you can use  \n",
    ">\n",
    ">       np.hstack() \n",
    ">\n",
    ">   to append a column of ones to an existing matrix (e.g., to construct the $\\mathbf{X}$ matrix in Equation (2.8) in the book).\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d91d26b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Viewer at 0x1667306e0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Crearnos una matriz X e Y, uqe van estar formadas por los landmark en world coordinate que hemos elegido anetriormente \n",
    "# Step 2: Calcular M = YX^T(XX^t)^-1\n",
    "#Step 3: calcular la ecuacion que nos dan \n",
    "\n",
    "# T1 and T2 voxel coordinates from the landmarks you've provided\n",
    "T1_landmarks_vox = np.array([\n",
    "    [20, 85, 75],  # Nose\n",
    "    [233, 124, 52],  # Nuca alta\n",
    "    [187, 66, 106],  # Nuca baja\n",
    "    [55, 10, 82],  # Barbilla\n",
    "    [52, 166, 86]  # Frente\n",
    "])\n",
    "\n",
    "T2_landmarks_vox = np.array([\n",
    "    [120, 221, 8],  # Nose\n",
    "    [132, 35, 22],  # Nuca alta\n",
    "    [132, 34, 1],  # Nuca baja\n",
    "    [126, 223, 2],  # Barbilla\n",
    "    [121, 204, 23]  # Frente\n",
    "])\n",
    "\n",
    "# Convert voxel coordinates to world coordinates using affine matrices\n",
    "T1_landmarks_world = (T1_affine @ np.hstack((T1_landmarks_vox, np.ones((T1_landmarks_vox.shape[0], 1)))).T).T[:, :3]\n",
    "T2_landmarks_world = (T2_affine @ np.hstack((T2_landmarks_vox, np.ones((T2_landmarks_vox.shape[0], 1)))).T).T[:, :3]\n",
    "\n",
    "# Construct the X and Y matrices for the affine transformation computation\n",
    "X = np.hstack([T1_landmarks_world, np.ones((T1_landmarks_world.shape[0], 1))]).T  # Shape (4, N)\n",
    "Y = np.hstack([T2_landmarks_world, np.ones((T2_landmarks_world.shape[0], 1))]).T  # Shape (4, N)\n",
    "\n",
    "# Compute the affine matrix M using Equation (2.8)\n",
    "M = Y @ X.T @ np.linalg.inv(X @ X.T)\n",
    "\n",
    "# Step 2: Apply the affine transformation to resample the T2 image onto the T1 grid\n",
    "# Using the equation: v_T2 = M_T2^-1 * M * M_T1 * v_T1\n",
    "\n",
    "# Step 2.1: Create the coordinate grid for T1\n",
    "V1, V2, V3 = np.meshgrid(np.arange(T1_data.shape[0]), \n",
    "                         np.arange(T1_data.shape[1]), \n",
    "                         np.arange(T1_data.shape[2]), indexing='ij')\n",
    "T1_voxel_coords = np.vstack([V1.ravel(), V2.ravel(), V3.ravel(), np.ones(V1.size)])\n",
    "\n",
    "# Step 2.2: Compute the transformation matrix from T1 to T2 using the affine matrix M\n",
    "T1_to_T2_affine = np.linalg.inv(T2_affine) @ M @ T1_affine\n",
    "\n",
    "# Step 2.3: Transform the T1 voxel coordinates into T2 voxel coordinates\n",
    "T2_voxel_coords = T1_to_T2_affine @ T1_voxel_coords\n",
    "T2_voxel_coords = T2_voxel_coords[:3]  # Discard the homogeneous coordinate (the 4th row)\n",
    "\n",
    "# Step 3: Interpolate the T2 data at these T2 voxel coordinates using cubic interpolation\n",
    "T2_data_resampled = scipy.ndimage.map_coordinates(T2_data, T2_voxel_coords, order=3).reshape(T1_data.shape)\n",
    "\n",
    "# Normalize both volumes to be in [0, 1] range\n",
    "T2_data_resampled_normalized = T2_data_resampled / T2_data_resampled.max()\n",
    "T1_data_normalized = T1_data / T1_data.max()\n",
    "\n",
    "# Step 5: Visualize the overlay\n",
    "Viewer(T2_data_resampled_normalized + T1_data_normalized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c262035",
   "metadata": {},
   "source": [
    "The code itself explains step by step what we are doing. \n",
    "By increasing the number of landmarks it would improve the accuracy of the transformation (this is only if the new landmarks are spread across the entire volume taking into acount the measures in task 3). On the other hand decreasing the number of said landmarks makes the registrarion less reliable as it is giving more weight to each of the few points we selected. It ight cause some misalignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ea80f0",
   "metadata": {},
   "source": [
    "### Task 5: Perform rigid landmark-based registration \n",
    "\n",
    "Repeat Task 4, but this time using a *rigid* transformation model. Vary the number of landmarks that are used again, and comment. Which transformation model (affine or rigid) is more appropriate to use in this specific application?\n",
    "\n",
    "> ***Hint:***\n",
    "> - a singular value decomposition can be computed using\n",
    ">           \n",
    ">          np.linalg.svd()\n",
    ">\n",
    "> - the determinant of a matrix can be computed using\n",
    ">    \n",
    ">         np.linalg.det()\n",
    ">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839060ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Landmarks for T1 (source) and T2 (target) in world coordinates (convert if needed)\n",
    "#We could have used the already defined ones, but just to make sure \n",
    "\n",
    "T1_landmarks_world = (T1_affine @ np.hstack((T1_landmarks_vox, np.ones((T1_landmarks_vox.shape[0], 1)))).T).T[:, :3]\n",
    "T2_landmarks_world = (T2_affine @ np.hstack((T2_landmarks_vox, np.ones((T2_landmarks_vox.shape[0], 1)))).T).T[:, :3]\n",
    "\n",
    "# Step 2: Compute the mean of both sets of landmarks\n",
    "T1_mean = np.mean(T1_landmarks_world, axis=0)\n",
    "T2_mean = np.mean(T2_landmarks_world, axis=0)\n",
    "\n",
    "# Step 3: Center the landmarks by subtracting the centroids\n",
    "T1_centered = T1_landmarks_world - T1_mean\n",
    "T2_centered = T2_landmarks_world - T2_mean\n",
    "\n",
    "# Step 4: Compute the cross-covariance matrix H\n",
    "H = T1_centered.T @ T2_centered\n",
    "\n",
    "# Step 5: Perform SVD on the cross-covariance matrix\n",
    "U, S, Vt = np.linalg.svd(H)\n",
    "\n",
    "# Step 6: Compute the rotation matrix R\n",
    "R = Vt.T @ U.T\n",
    "\n",
    "# Step 7: Ensure a right-handed coordinate system (determinant of R must be +1)\n",
    "if np.linalg.det(R) < 0:\n",
    "    Vt[-1, :] *= -1  # Flip the last column of Vt to ensure a positive determinant\n",
    "    R = Vt.T @ U.T\n",
    "\n",
    "# Step 8: Compute the translation vector t\n",
    "t = T2_mean - R @ T1_mean\n",
    "\n",
    "# Now, we have the rigid transformation: Rotation (R) and Translation (t)\n",
    "\n",
    "# Step 9: Transform the T1 voxel grid to T2 using the rigid transformation\n",
    "\n",
    "# Create a voxel grid for T1 (as done in earlier examples)\n",
    "V1, V2, V3 = np.meshgrid(\n",
    "    np.arange(T1_data.shape[0]),\n",
    "    np.arange(T1_data.shape[1]),\n",
    "    np.arange(T1_data.shape[2]),\n",
    "    indexing='ij'\n",
    ")\n",
    "\n",
    "# Stack voxel coordinates into homogeneous coordinates (N, 4) format\n",
    "T1_voxel_coords = np.vstack([V1.ravel(), V2.ravel(), V3.ravel(), np.ones(V1.size)])\n",
    "\n",
    "# Convert T1 voxel coordinates to world coordinates\n",
    "T1_world_coords = T1_affine @ T1_voxel_coords\n",
    "\n",
    "# Step 10: Apply the rigid transformation (Rotation + Translation)\n",
    "T1_transformed_world_coords = R @ T1_world_coords[:3, :] + t[:, np.newaxis]\n",
    "\n",
    "# Step 11: Convert the transformed world coordinates back to T2 voxel coordinates\n",
    "T2_voxel_coords = np.linalg.inv(T2_affine) @ np.vstack([T1_transformed_world_coords, np.ones(T1_transformed_world_coords.shape[1])])\n",
    "\n",
    "# Step 12: Interpolate the T2 image at the transformed coordinates\n",
    "T2_voxel_coords = T2_voxel_coords[:3, :]\n",
    "T2_voxel_coords = T2_voxel_coords.reshape((3,) + T1_data.shape)  # Reshape to match the T1 volume shape\n",
    "\n",
    "# Use cubic interpolation to sample T2 data at the transformed coordinates\n",
    "T2_data_resampled = scipy.ndimage.map_coordinates(T2_data, T2_voxel_coords, order=3)\n",
    "\n",
    "# Step 13: Visualize the result\n",
    "# Normalize both images to the range [0, 1] for visualization\n",
    "T1_data_normalized = T1_data / T1_data.max()\n",
    "T2_data_resampled_normalized = T2_data_resampled / T2_data_resampled.max()\n",
    "\n",
    "# Combine the images for visualization\n",
    "overlay = T1_data_normalized + T2_data_resampled_normalized\n",
    "\n",
    "Rigid_image = Viewer (overlay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4482bd9c",
   "metadata": {},
   "source": [
    "The code itself explains step by step what we are doing.\n",
    "For this task we now used a rigid transformation to align the T1 weighted to the T2 weighted. Rigid transformation focuses on changing the position and orientation of the brain without changing its size or shape. In brain MRI it makes much more sense to use rigid transformation as the brain tipically doesn't change size or shape in a short period of time, this means that the brain structure stays the same so by using rigid transformation we make sure the brain images are actually aligned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2883b194",
   "metadata": {},
   "source": [
    "### : CONCLUSION\n",
    "\n",
    "The exercise made us help understand coordinate systems, transforming form voxel to world coordinate in task 1. We also learned how to resample MRI images and select siginificant landmarks which is vital for our future as biomedical engineers. We then performed both affine and rigid transformation and realized rigid transformation is better to use for brain MRI because it mantains the brain structure better. Overall we gained a comprehensive understanding on analyzind MRI data which is essential for accurately interpreting brain scans in medical imaging applications. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

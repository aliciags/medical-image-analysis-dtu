{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cfeae45-c6f1-48f7-8df8-34851c8d9acc",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "<p style=\"text-align: justify;\">\n",
    "This notebook focuses on MRI (Magnetic Resonance Imaging) is a critical tool in medical imaging, providing high-resolution, non-invasive scans of soft tissues like the brain. Different types of MRI scans, such as T1-weighted and T2-weighted, highlight different tissue properties, and accurate analysis of these images is crucial for diagnosing and monitoring various conditions. The notebook covers the following concepts.\n",
    "</p>\n",
    "\n",
    "* Differentiating between voxel and world coordinates for accurate image interpretation, hence understanding the difference between the coordinate systems.\n",
    "* Using a viewer to explore 3D MRI volumes.\n",
    "* Aligning MRI datasets through affine and rigid transformations, where rigid is preferred for brain scans as it preserves size and shape.\n",
    "* Matching voxel grids between different MRI datasets for comparison.\n",
    "\n",
    "The goal is to enhance understanding of image alignment and resampling, which are crucial in medical imaging.\n",
    "\n",
    "```\n",
    "pip install ipython\n",
    "```\n",
    "\n",
    "This installation is required for the Viewers detailed later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84580e68",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88abf658-92bc-4c5b-adcb-3c43a02c451d",
   "metadata": {},
   "source": [
    "## Input data and code hints\n",
    "Import Python libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c00a4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib tk\n",
    "plt.ion()\n",
    "import numpy as np\n",
    "np.set_printoptions( suppress=True )\n",
    "import nibabel as nib\n",
    "import scipy\n",
    "import IPython\n",
    "# IPython.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3273d999-6015-4e04-ba0f-103a683fb5bb",
   "metadata": {},
   "source": [
    "Read the two 3D scans you'll be working with in this exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aafeca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load T1 and T2 data\n",
    "T1_fileName = 'IXI014-HH-1236-T1.nii.gz'\n",
    "T2_fileName = 'IXI014-HH-1236-T2_moved.nii.gz'\n",
    "T1 = nib.load( T1_fileName )\n",
    "T2 = nib.load( T2_fileName )\n",
    "T1_data = T1.get_fdata()\n",
    "T2_data = T2.get_fdata()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303e356d-295a-456b-9984-baf0bc573ce6",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify;\">\n",
    "Below is code to define a simple interactive viewer class that can be used to visualize 2D cross-sections of a 3D array along three orthogonal directions. It takes a 3D volume as input and shows the location a \"linked cursor\" in all three cross-sections.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e8646cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Viewer:\n",
    "    def __init__(self, data ):\n",
    "        self.fig, self.ax = plt.subplots()\n",
    "        self.data = data\n",
    "        self.dims = self.data.shape\n",
    "        self.position = np.round( np.array( self.dims ) / 2 ).astype( int )\n",
    "        self.draw()\n",
    "        self.fig.canvas.mpl_connect( 'button_press_event', self )\n",
    "        self.fig.show()\n",
    "\n",
    "    def __call__(self, event):\n",
    "        print( 'button pressed' )\n",
    "        if event.inaxes is None: return\n",
    "      \n",
    "        x, y = round( event.xdata ), round( event.ydata )\n",
    "\n",
    "        #\n",
    "        if ( x > (self.dims[0]-1) ) and ( y <= (self.dims[1]-1) ): return # lower-right quadrant\n",
    "          \n",
    "        #\n",
    "        if x < self.dims[0]:\n",
    "          self.position[ 0 ] = x\n",
    "        else:\n",
    "          self.position[ 1 ] = x - self.dims[0]\n",
    "        \n",
    "        if y < self.dims[1]:\n",
    "          self.position[ 1 ] = y\n",
    "        else:\n",
    "          self.position[ 2 ] = y -self.dims[1]\n",
    "        \n",
    "        print( f\"  voxel index: {self.position}\" )\n",
    "        print( f\"  intensity: {self.data[ self.position[0], self.position[1], self.position[2] ]}\" )\n",
    "\n",
    "        self.draw()\n",
    "\n",
    "    def draw( self ):\n",
    "        #\n",
    "        # Layout on screen is like this:\n",
    "        #\n",
    "        #     ^            ^\n",
    "        #  Z  |         Z  |\n",
    "        #     |            |\n",
    "        #     ----->        ---->  \n",
    "        #       X             Y\n",
    "        #     ^\n",
    "        #  Y  |\n",
    "        #     |\n",
    "        #     ----->  \n",
    "        #       X\n",
    "        #\n",
    "        dims = self.dims\n",
    "        position = self.position\n",
    "        \n",
    "        xySlice = self.data[ :, :, position[ 2 ] ]\n",
    "        xzSlice = self.data[ :, position[ 1 ], : ]\n",
    "        yzSlice = self.data[ position[ 0 ], :, : ]\n",
    "        \n",
    "        kwargs = dict( vmin=self.data.min(), vmax=self.data.max(), \n",
    "                       origin='lower', \n",
    "                       cmap='gray',\n",
    "                       picker=True )\n",
    "\n",
    "        self.ax.clear()\n",
    "\n",
    "        self.ax.imshow( xySlice.T, \n",
    "                        extent=( 0, dims[0]-1, \n",
    "                                 0, dims[1]-1 ), \n",
    "                        **kwargs )\n",
    "        self.ax.imshow( xzSlice.T, \n",
    "                        extent=( 0, dims[0]-1, \n",
    "                                 dims[1], dims[1]+dims[2]-1 ), \n",
    "                        **kwargs )\n",
    "        self.ax.imshow( yzSlice.T, extent=( dims[0], dims[0]+dims[1]-1, \n",
    "                                            dims[1], dims[1]+dims[2]-1 ), \n",
    "                        **kwargs )\n",
    "\n",
    "        color = 'g'\n",
    "        self.ax.plot( (0, dims[0]-1), (position[1], position[1]), color )\n",
    "        self.ax.plot( (0, dims[0]+dims[1]-1), (dims[1]+position[2], dims[1]+position[2]), color )\n",
    "        self.ax.plot( (position[0], position[0]), (0, dims[1]+dims[2]-1), color )\n",
    "        self.ax.plot( (dims[0]+position[1], dims[0]+position[1]), (dims[1]+1, dims[1]+dims[2]-1), color )\n",
    "\n",
    "        self.ax.set( xlim=(1, dims[0]+dims[1]), ylim=(0, dims[1]+dims[2]) )\n",
    "\n",
    "        self.ax.text( dims[0] + dims[1]/2, dims[1]/2, \n",
    "                      f\"voxel index: {position}\",  \n",
    "                      horizontalalignment='center', verticalalignment='center' )\n",
    "  \n",
    "        self.ax.axis( False )\n",
    "\n",
    "        self.fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccd2de5-8803-4f20-ad11-995425864206",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify;\">\n",
    "The code below shows how to visualize the T1-weigthed and the T2-weighted volumes with this viewer class. The initial location of the cursor is in the middle of the volume in each case. It can be changed by clicking on one of the cross-sections. The viewer also displays the voxel index $\\mathbf{v}$ of the cursor. Play around and try to understand what the Viewer() class does.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2045427-0c33-4b62-a0f2-adfa98fdffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "T1_viewer = Viewer( T1_data )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2185e81",
   "metadata": {},
   "source": [
    "![CRANE_VOXEL_T1](./pictures/Figure_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a4b223d-0be2-43a6-bbcf-dfe08c6abd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "T2_viewer = Viewer( T2_data )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3873889",
   "metadata": {},
   "source": [
    "![CRANE_VOXEL_T2](./pictures/Figure_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc48f02",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c262d189",
   "metadata": {},
   "source": [
    "# Task 1: Coordinate Systems\n",
    "\n",
    "Familiarize yourself with the concept of coordinate systems. In your report, explain why we need to differentiate between \"voxel coordinates/indices\" $\\mathbf{v}$ and \"world coordinates\" $\\mathbf{x}$. Why does the T2-weighted volume look so compressed in the viewer? For the enthusiastic student: calculate the voxel size in each dataset.\n",
    "\n",
    "The world coordinate system used in both the T1-weighted and the T2-weighted scan follows the RAS convention. Equipped with this information, determine the voxel index $\\mathbf{v}$ of the center of the left eye of the patient in the T1-weighted scan. Do the same for the T2-weighted scan.\n",
    " \n",
    "> ***Hints:***\n",
    "> \n",
    "> - The affine voxel-to-world matrix of the T1-weighted scan is given by\n",
    ">\n",
    ">        T1.affine\n",
    ">\n",
    "> \n",
    "> - In nibabel, the RAS convention is used (see bottom of https://nipy.org/nibabel/coordinate_systems.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2819fd67",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify;\">\n",
    "The voxel coordinates/indices reffer to the 3 different numbers, usually i,j,k; they are the grid indices of individual volume elements. \n",
    "<br>\n",
    "The world coordinates represents the actual topology of the patient. We could think that both coordinates always have the same ratio, but actually it does not. \n",
    "<br>\n",
    "The MRI scan is not always isotropic, the spacing between voxels differs across the three dimensions. \n",
    "<br>\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align: justify;\">\n",
    "T2 looks compressed in the viewer because the spacing between voxels in the slice plane (e.g., x and y dimensions) is often much smaller than the slice thickness (z-dimension), resulting in the appearance of compression or stretching along the z-axis when visualized (it is an MRI scan). \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "183eec30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voxel size of T1: [0.01506851 0.12376414 0.01646696] mm\n",
      "Voxel size of T2: [0.83986628 0.86080861 4.79742765] mm\n",
      "World coordinates of T1 for the left patient eye will be:  [32.75329093 75.34755456 -0.436423  ] mm\n",
      "World coordinates of T2 for the left patient eye will be:  [-19.56145275 116.24901803   3.76028705] mm\n"
     ]
    }
   ],
   "source": [
    "# Voxel size\n",
    "T1_affine = T1.affine\n",
    "# print (T1_affine)\n",
    "T2_affine = T2.affine\n",
    "\n",
    "voxel_size_T1= np.abs(np.diag(T1_affine)[:3]) \n",
    "print (f\"Voxel size of T1: {voxel_size_T1} mm\")\n",
    "\n",
    "voxel_size_T2= np.abs(np.diag(T2_affine)[:3])\n",
    "print (f\"Voxel size of T2: {voxel_size_T2} mm\")\n",
    "\n",
    "# Voxel index v of the center of the left eye of the patient \n",
    "# x= Av +t where x is the world coordinate, v is the voxel index, A is the affine matrix and t is the translation vector\n",
    "# A^-1(x -t) = v  where A^-1 is the inverse of the affine matrix\n",
    "\n",
    "A = np.linalg.inv(T1.affine[:3,:3])  \n",
    "t= T1_affine[:3,3]\n",
    "# print(t)\n",
    "voxel_indx =[64, 120, 103,1] # Left eye of the patient \n",
    "# print (voxel_indx)\n",
    "x = T1_affine @ voxel_indx\n",
    "print (\"World coordinates of T1 for the left patient eye will be: \",x[:3] ,\"mm\") \n",
    "\n",
    "A = np.linalg.inv(T2.affine[:3,:3])  \n",
    "t= T2_affine[:3,3]\n",
    "# print(t)\n",
    "voxel_indx =[141, 216, 15,1] # Left eye of the patient \n",
    "# print (voxel_indx)\n",
    "x = T2_affine @ voxel_indx\n",
    "print (\"World coordinates of T2 for the left patient eye will be: \",x[:3] ,\"mm\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc112c3",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify;\">\n",
    "For this task it is very important to know the difference between world coordinates and voxel coordinates. Voxel coordinates refer to the position of a voxel in a 3D grid (so it has i, j, k) that makes up the MRI volume. World coordinates, on the other hand, represent the actual position of a voxel in the patient’s body and are expressed in millimeters. To transform voxel coordinates to world coordinates we use ``x = Av + t`` as stated on the slides. The affine matrix has important information regarding scaling, rotation and translation. Being the diagonal of the matrix (except the element in position 4,4 which is a 1) the voxel size. We then got the world coordinates from the voxel coordinates that we measured in the left eye by using the same formula but with the inverse of the affine matrix.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d32f21",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4532ea",
   "metadata": {},
   "source": [
    "# Task 2: Resample the T2-weighted scan to the image grid of the T1-weighted scan\n",
    "\n",
    "In this task you should resample the T2-weighted scan to the image grid of the T1-weighted scan, i.e., create a new 3D volume that has the same size as the T1-weighted volume, but that contains interpolated T2-weighted intensities instead. In particular, for each voxel index $\\mathbf{v}_{T1}$ in the T1-weighted image grid, you should compute the corresponding voxel index $\\mathbf{v}_{T2}$ in the T2-weighted volume as follows (see section 2.1 in the book):\n",
    "$$\n",
    "\\begin{pmatrix} \\mathbf{ v_{T2}} \\\\ 1 \\end{pmatrix} = \\mathbf{M}_{T2}^{-1} \\cdot \\mathbf{M}_{T1} \\cdot \\begin{pmatrix} \\mathbf{ v_{T1}} \\\\ 1 \\end{pmatrix}\n",
    ".\n",
    "$$\n",
    "At the location $\\mathbf{v}_{T2}$, you should then use cubic B-spline interpolation to determine the intensity in the T2-weighted scan, and store it at index $\\mathbf{v}_{T1}$ in the newly created image.\n",
    "\n",
    "Once you have created a new volume like this, visualize it overlaid on the T1-weighted volume as follows:\n",
    "    \n",
    "    Viewer( T2_data_resampled / T2_data_resampled.max() + T1_data / T1_data.max() )\n",
    "\n",
    "> ***Hints:***\n",
    "> - you can create a coordinate grid in 3D with the function\n",
    "> \n",
    ">        V1,V2,V3 = np.meshgrid( np.arange( T1_data.shape[0] ), \n",
    ">                                np.arange( T1_data.shape[1] ), \n",
    ">                                np.arange( T1_data.shape[2] ), indexing='ij' )\n",
    ">   \n",
    ">\n",
    "> - the following SciPy function interpolates the T2-weighted volume at voxel coordinates $(1.1,2.2,3.3)^T$ \n",
    "> and $(6.6,7.7,8.8)^T$ using cubic interpolation:   \n",
    ">\n",
    ">        scipy.ndimage.map_coordinates( T2_data, np.array( [ [1.1,2.2,3.3], [6.6,7.7,8.8] ] ).T )\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d418bcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "V1,V2,V3 = np.meshgrid( np.arange( T1_data.shape[0] ), \n",
    "                         np.arange( T1_data.shape[1] ), \n",
    "                         np.arange( T1_data.shape[2] ), indexing='ij' )\n",
    "T1_voxel_coords = np.vstack([V1.ravel(), V2.ravel(), V3.ravel(), np.ones(V1.size)])\n",
    "\n",
    "T2_voxel_coords_n = np.linalg.inv(T2_affine) @ T1_affine @T1_voxel_coords\n",
    "T2_voxel_coords = T2_voxel_coords_n[:3,:]\n",
    "\n",
    "T2_data_resampled = scipy.ndimage.map_coordinates(T2_data, T2_voxel_coords, order=3).reshape(T1_data.shape)\n",
    "T2_viewer_r = Viewer( T2_data_resampled / T2_data_resampled.max() + T1_data / T1_data.max() )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99004f98",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify;\">\n",
    "In summary we used MT1 and MT2, the affine matrices for the T1 and T2 scans and used the formula given to find the location in the T2 scan for each voxel in the T1 scan.\n",
    "We first created the meshgrid that represent the voxel indices in T1. We  then stack them  into a matrix after using ravel which transforms a 3D array into a 1D array and we also addeed an extra row of 1s. We the create a matrix where each column has a 3D voxel coordinate in T2 and we take the first 3 rows to have te actual T2 voxel coordinates without the ones. Then we did a cubic B spline interpolation of order 3 to know the values of the voxels that are not integers and then reshaped into T1 dimensions.\n",
    "Finally we normalized by dividing by their max value so we get intensities that are between 0 and 1 and overlaid the two volumes so we can actually see the T2-weitghted scan onto the T1.\n",
    "</p>\n",
    "\n",
    "In the next image we can see the resample the T2-weighted scan to the image grid of the T1-weighted scan.\n",
    "\n",
    "![CRANE_VOXEL_T1_T2](./pictures/Figure_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fdc9b4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4165b671",
   "metadata": {},
   "source": [
    "# Task 3: Collect corresponding landmarks\n",
    "\n",
    "Using the viewer class, record the voxel coordinates $\\mathbf{v_{T1}}$ and $\\mathbf{v_{T2}}$ of at least five corresponding landmarks in the T1- and the T2-weighted volumes, respectively. List them in your report, and explain why you picked them. \n",
    "\n",
    "> ***Hint:***\n",
    "> - Avoid picking landmarks that are very close to each other or that all lie approximately in the same 2D plane.\n",
    "> - You can double-check which landmarks you've selected as follows:\n",
    ">         T1_viewer = Viewer( T1_data )\n",
    ">         T1_viewer.position = ( 20, 30, 40 )\n",
    ">         T1_viewer.draw()\n",
    "> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed33eda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set 1 of Landmarks\n",
    "# T1 set 1 \n",
    "Landmarks_T1_1= np.array([\n",
    "[20, 85, 75], #Nose\n",
    "[233, 124, 52], #Upper head\n",
    "[187, 66, 106], #Lower head\n",
    "[55, 10, 82], #Chin\n",
    "[52, 166, 86] #Forehead\n",
    "])\n",
    "\n",
    "# T2 set 1\n",
    "Landmarks_T2_1= np.array ([\n",
    "[120, 221, 8],\n",
    "[132, 35, 22],\n",
    "[132, 34, 1],\n",
    "[126, 223, 2],\n",
    "[121, 204, 23]\n",
    "])\n",
    "\n",
    "#Set 2 of Landmarks\n",
    "#T1 set 2: \n",
    "Landmarks_T1_2= np.array([\n",
    "[61, 114, 107], #Right eye\n",
    "[218, 163, 46], #Upper head\n",
    "[210, 52, 65], #Lower head\n",
    "[153, 96, 21], #Right ear\n",
    "[51, 174, 78] #Forehead\n",
    "])\n",
    "\n",
    "# T2 set 2:\n",
    "Landmarks_T2_2= np.array([\n",
    "[18, 114, 106],\n",
    "[185, 147, 95],\n",
    "[172, 23, 109],\n",
    "[126, 223, 2],\n",
    "[86, 83, 35]\n",
    "])\n",
    "\n",
    "#Set 3 of Landmarks\n",
    "#T1 set 3:\n",
    "Landmarks_T1_3 = np.array ([ \n",
    "[11, 78, 812], # Tip of nose\n",
    "[62, 114, 101], # Right eye \n",
    "[59, 108, 53], # Left eye  \n",
    "[220, 100, 73], # Back of head  \n",
    "[127, 182, 77] # Top of head\n",
    "])\n",
    "\n",
    "#T2 set 3:                        \n",
    "Landmarks_T2_3= np.array([\n",
    "[121, 245, 51], # Tip of nose \n",
    "[89, 201, 83], # Right eye \n",
    "[150, 203, 9], # Left eye  \n",
    "[130, 41, 103], # Back of head \n",
    "[127, 120, 27] # Top of head\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed076602",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify;\">\n",
    "For this part we just had to select landmarks using the viewer taking into acount that they had to be evenly distributed, anatomically recognizable and in different planes to provide a good 3D view.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba2cb5f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fed8836",
   "metadata": {},
   "source": [
    "# Task 4: Perform affine landmark-based registration \n",
    "\n",
    "Using the landmarks you recorded in the previous task, compute the parameters of the 3D affine transformation that brings the landmarks in the T1-weighted image closest to the corresponding ones in the T2-weighted image. For this purpose, use Equation (2.8) in the book.\n",
    "\n",
    "Once you've determined the affine transformation, register to two images by resampling the T2-weighted image to the image grid of the T1-weighted image, and overlay the two images as in Task 2. To map voxel coordinates $\\mathbf{v}_T1$ to $\\mathbf{v}_T2$, you'll have to use \n",
    "$$\n",
    "\\begin{pmatrix} \\mathbf{ v_{T2}} \\\\ 1 \\end{pmatrix} = \\mathbf{M}_{T2}^{-1} \\cdot \\mathbf{M} \\cdot \\mathbf{M}_{T1} \\cdot \\begin{pmatrix} \\mathbf{ v_{T1}} \\\\ 1 \\end{pmatrix}\n",
    ",\n",
    "$$\n",
    "where $\\mathbf{M}$ is your $4 \\times 4$ affine matrix (see book).\n",
    "\n",
    "What happens when you increase/decrease the number of corresponding landmarks that are used in the computations? Comment.\n",
    "\n",
    "> ***Hint:***\n",
    "> - remember that the affine matrix works in *world* coordinates, so you'll have to map your landmarks to world coordinates first.\n",
    "> - you can use  \n",
    ">\n",
    ">       np.hstack() \n",
    ">\n",
    ">   to append a column of ones to an existing matrix (e.g., to construct the $\\mathbf{X}$ matrix in Equation (2.8) in the book).\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d91d26b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing set 1 of landmarks...\n",
      "Processing set 2 of landmarks...\n",
      "Processing set 3 of landmarks...\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create a matrix X & Y, that holds the landmark in world coordinate␣ ↪chosen before\n",
    "# Step 2: Calculate M = YX^T(XX^t)^-1\n",
    "# Step 3: Calculate the provided equation\n",
    "\n",
    "landmark_sets_T1 = [Landmarks_T1_1, Landmarks_T1_2, Landmarks_T1_3]\n",
    "landmark_sets_T2 = [Landmarks_T2_1, Landmarks_T2_2, Landmarks_T2_3]\n",
    "\n",
    "# Loop over each set of landmarks\n",
    "for i, (Landmarks_T1, Landmarks_T2) in enumerate(zip(landmark_sets_T1, landmark_sets_T2)):\n",
    "    print(f\"Processing set {i+1} of landmarks...\")\n",
    "    \n",
    "    # Convert voxel coordinates to world coordinates using affine matrices for each set\n",
    "    T1_landmarks_world = (T1_affine @ np.hstack((Landmarks_T1, np.ones((Landmarks_T1.shape[0], 1)))).T).T[:, :3]\n",
    "    T2_landmarks_world = (T2_affine @ np.hstack((Landmarks_T2, np.ones((Landmarks_T2.shape[0], 1)))).T).T[:, :3]\n",
    "\n",
    "    # Construct the X and Y matrices for the affine transformation computation\n",
    "    X = np.hstack([T1_landmarks_world, np.ones((T1_landmarks_world.shape[0], 1))]).T  # Shape (4, N)\n",
    "    Y = np.hstack([T2_landmarks_world, np.ones((T2_landmarks_world.shape[0], 1))]).T  # Shape (4, N)\n",
    "\n",
    "    # Compute the affine matrix M using Equation (2.8)\n",
    "    M = Y @ X.T @ np.linalg.inv(X @ X.T)\n",
    "\n",
    "    # Apply the affine transformation to resample the T2 image onto the T1 grid\n",
    "    # Using the equation: v_T2 = M_T2^-1 * M * M_T1 * v_T1\n",
    "\n",
    "    # Create the coordinate grid for T1\n",
    "    V1, V2, V3 = np.meshgrid(np.arange(T1_data.shape[0]), \n",
    "                             np.arange(T1_data.shape[1]), \n",
    "                             np.arange(T1_data.shape[2]), indexing='ij')\n",
    "    T1_voxel_coords = np.vstack([V1.ravel(), V2.ravel(), V3.ravel(), np.ones(V1.size)])\n",
    "\n",
    "    # Compute the transformation matrix from T1 to T2 using the affine matrix M\n",
    "    T1_to_T2_affine = np.linalg.inv(T2_affine) @ M @ T1_affine\n",
    "\n",
    "    # Transform the T1 voxel coordinates into T2 voxel coordinates\n",
    "    T2_voxel_coords = T1_to_T2_affine @ T1_voxel_coords\n",
    "    T2_voxel_coords = T2_voxel_coords[:3]  # Discard the homogeneous coordinate (the 4th row)\n",
    "\n",
    "    # Interpolate the T2 data at these T2 voxel coordinates using cubic interpolation\n",
    "    T2_data_resampled = scipy.ndimage.map_coordinates(T2_data, T2_voxel_coords, order=3).reshape(T1_data.shape)\n",
    "\n",
    "    # Normalize both volumes to be in [0, 1] range\n",
    "    T2_data_resampled_normalized = T2_data_resampled / T2_data_resampled.max()\n",
    "    T1_data_normalized = T1_data / T1_data.max()\n",
    "\n",
    "    # Visualize the overlay\n",
    "    Viewer(T2_data_resampled_normalized + T1_data_normalized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c262035",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify;\">\n",
    "The code comments present every step of the process.\n",
    "<br>\n",
    "By increasing the number of landmarks it would improve the accuracy of the transformation (this is only if the new landmarks are spread across the entire volume taking into acount the measures in task 3). On the other hand decreasing the number of said landmarks makes the registrarion less reliable as it is giving more weight to each of the few points we selected. It might cause some misalignment.\n",
    "</p>\n",
    "\n",
    "For each set of landmarks we get the following results.\n",
    "\n",
    "First set of chosen landmarks\n",
    "<br>\n",
    "\n",
    "![FIRST_SET_OF_LANDMARKS](./pictures/Figure_4_LANDMARK%201.png)\n",
    "\n",
    "Second set of chosen landmarks\n",
    "<br>\n",
    "\n",
    "![SECOND_SET_OF_LANDMARKS](./pictures/Figure_5_LANDMARK2.png)\n",
    "\n",
    "Third set of chosen landmarks\n",
    "<br>\n",
    "\n",
    "![THIRD_SET_OF_LANDMARKS](./pictures/Figure_6_LANDMARK3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908b8f72",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ea80f0",
   "metadata": {},
   "source": [
    "# Task 5: Perform rigid landmark-based registration \n",
    "\n",
    "Repeat Task 4, but this time using a *rigid* transformation model. Vary the number of landmarks that are used again, and comment. Which transformation model (affine or rigid) is more appropriate to use in this specific application?\n",
    "\n",
    "> ***Hint:***\n",
    "> - a singular value decomposition can be computed using\n",
    ">           \n",
    ">          np.linalg.svd()\n",
    ">\n",
    "> - the determinant of a matrix can be computed using\n",
    ">    \n",
    ">         np.linalg.det()\n",
    ">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "839060ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing landmark set 1...\n",
      "Processing with 3 landmarks...\n",
      "Displaying overlay for set 1 with 3 landmarks...\n",
      "Processing with 4 landmarks...\n",
      "Displaying overlay for set 1 with 4 landmarks...\n",
      "Processing with 5 landmarks...\n",
      "Displaying overlay for set 1 with 5 landmarks...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Function to select a subset of landmarks\n",
    "def select_landmarks(landmarks_T1, landmarks_T2, num_landmarks):\n",
    "    idx = np.random.choice(landmarks_T1.shape[0], num_landmarks, replace=False)  # Randomly select indices\n",
    "    return landmarks_T1[idx], landmarks_T2[idx]\n",
    "\n",
    "# Define the sets of Landmarks used for this task \n",
    "# To optimize the computational time, we are only going to do so with our first set. \n",
    "# Uncomment the following code lines if you want to display the images for the three different sets: \n",
    "# landmark_sets_T1= [Landmarks_T1_1, Landmarks_T1_2,Landmarks_T1_3]\n",
    "# landmark_sets_T2= [Landmarks_T2_1, Landmarks_T2_2,Landmarks_T2_3]\n",
    "landmark_sets_T1 = [Landmarks_T1_1]\n",
    "landmark_sets_T2 = [Landmarks_T2_1]\n",
    "\n",
    "\n",
    "# Number of landmarks to use (3, 4, 5)\n",
    "num_landmarks_list = [3, 4, 5 ]\n",
    "\n",
    "# Loop over each set of landmarks\n",
    "for i, (Landmarks_T1, Landmarks_T2) in enumerate(zip(landmark_sets_T1, landmark_sets_T2)):\n",
    "    print(f\"Processing landmark set {i+1}...\")\n",
    "    \n",
    "    # Loop over different numbers of landmarks\n",
    "    for num_landmarks in num_landmarks_list:\n",
    "        print(f\"Processing with {num_landmarks} landmarks...\")\n",
    "\n",
    "        # Select subsets of landmarks from the provided ones\n",
    "        T1_landmarks_subset, T2_landmarks_subset = select_landmarks(Landmarks_T1, Landmarks_T2, num_landmarks)\n",
    "\n",
    "        # Convert voxel coordinates to world coordinates using affine matrices\n",
    "        T1_landmarks_world = (T1_affine @ np.hstack((T1_landmarks_subset, np.ones((T1_landmarks_subset.shape[0], 1)))).T).T[:, :3]\n",
    "        T2_landmarks_world = (T2_affine @ np.hstack((T2_landmarks_subset, np.ones((T2_landmarks_subset.shape[0], 1)))).T).T[:, :3]\n",
    "\n",
    "        # Compute the mean of both sets of landmarks\n",
    "        T1_mean = np.mean(T1_landmarks_world, axis=0)\n",
    "        T2_mean = np.mean(T2_landmarks_world, axis=0)\n",
    "\n",
    "        # Center the landmarks by subtracting the centroids\n",
    "        T1_centered = T1_landmarks_world - T1_mean\n",
    "        T2_centered = T2_landmarks_world - T2_mean\n",
    "\n",
    "        # Compute the cross-covariance matrix H\n",
    "        H = T1_centered.T @ T2_centered\n",
    "\n",
    "        # Perform SVD on the cross-covariance matrix\n",
    "        U, S, Vt = np.linalg.svd(H)\n",
    "\n",
    "        # Compute the rotation matrix R\n",
    "        R = Vt.T @ U.T\n",
    "\n",
    "        # Ensure a right-handed coordinate system (determinant of R must be +1)\n",
    "        if np.linalg.det(R) < 0:\n",
    "            Vt[-1, :] *= -1  # Flip the last column of Vt to ensure a positive determinant\n",
    "            R = Vt.T @ U.T\n",
    "\n",
    "        # Compute the translation vector t\n",
    "        t = T2_mean - R @ T1_mean\n",
    "\n",
    "        # Now, we have the rigid transformation: Rotation (R) and Translation (t)\n",
    "\n",
    "        # Transform the T1 voxel grid to T2 using the rigid transformation\n",
    "\n",
    "        # Create a voxel grid for T1 \n",
    "        V1, V2, V3 = np.meshgrid(\n",
    "            np.arange(T1_data.shape[0]),\n",
    "            np.arange(T1_data.shape[1]),\n",
    "            np.arange(T1_data.shape[2]),\n",
    "            indexing='ij'\n",
    "        )\n",
    "\n",
    "        # Stack voxel coordinates into homogeneous coordinates (N, 4) format\n",
    "        T1_voxel_coords = np.vstack([V1.ravel(), V2.ravel(), V3.ravel(), np.ones(V1.size)])\n",
    "\n",
    "        # Convert T1 voxel coordinates to world coordinates\n",
    "        T1_world_coords = T1_affine @ T1_voxel_coords\n",
    "\n",
    "        # Apply the rigid transformation (Rotation + Translation)\n",
    "        T1_transformed_world_coords = R @ T1_world_coords[:3, :] + t[:, np.newaxis]\n",
    "\n",
    "        # Convert the transformed world coordinates back to T2 voxel coordinates\n",
    "        T2_voxel_coords = np.linalg.inv(T2_affine) @ np.vstack([T1_transformed_world_coords, np.ones(T1_transformed_world_coords.shape[1])])\n",
    "\n",
    "        # Interpolate the T2 image at the transformed coordinates\n",
    "        T2_voxel_coords = T2_voxel_coords[:3, :]\n",
    "        T2_voxel_coords = T2_voxel_coords.reshape((3,) + T1_data.shape)  # Reshape to match the T1 volume shape\n",
    "\n",
    "        # Use cubic interpolation to sample T2 data at the transformed coordinates\n",
    "        T2_data_resampled = scipy.ndimage.map_coordinates(T2_data, T2_voxel_coords, order=3)\n",
    "\n",
    "        # Step 13: Visualize the result\n",
    "        # Normalize both images to the range [0, 1] for visualization\n",
    "        T1_data_normalized = T1_data / T1_data.max()\n",
    "        T2_data_resampled_normalized = T2_data_resampled / T2_data_resampled.max()\n",
    "\n",
    "        # Combine the images for visualization\n",
    "        overlay = T1_data_normalized + T2_data_resampled_normalized\n",
    "\n",
    "        # Display using the viewer \n",
    "        print(f\"Displaying overlay for set {i+1} with {num_landmarks} landmarks...\")\n",
    "        Viewer(overlay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4482bd9c",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify;\">\n",
    "The code comments present every step of the process.\n",
    "<br>\n",
    "For this task we now used a rigid transformation to align the T1 weighted to the T2 weighted. Rigid transformation focuses on changing the position and orientation of the brain without changing its size or shape. In brain MRI, it makes more sense to use rigid transformation as the brain tipically does not change size or shape in a short period of time, this means that the brain structure stays the same so by using rigid transformation we make sure the brain images are actually aligned.\n",
    "</p>\n",
    "\n",
    "The results of the rigid transformation of the first set of landmarks is shown below for the different number of landmarks.\n",
    "\n",
    "Three landmarks\n",
    "\n",
    "![RIGID_TRANSFORMATION_THREE_LANDMARKS](./pictures/Figure_7_THREE%20LANDMARKS.png)\n",
    "\n",
    "Four landmarks\n",
    "\n",
    "![RIGID_TRANSFORMATION_FOUR_LANDMARKS](./pictures/Figure_8_FOUR%20LANDMARKS.png)\n",
    "\n",
    "Five landmarks\n",
    "\n",
    "![RIGID_TRANSFORMATION_FIVE_LANDMARKS](./pictures/Figure_9_FIVE%20LANDMARKS.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2883b194",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "<p style=\"text-align: justify;\">\n",
    "The exercise made us help understand coordinate systems, transforming form voxel to world coordinate in task 1. We also learned how to resample MRI images and select siginificant landmarks which is vital for our future as biomedical engineers. We then performed both affine and rigid transformation and realized rigid transformation is better to use for brain MRI because it mantains the brain structure better. Overall we gained a comprehensive understanding on analyzing MRI data which is essential for accurately interpreting brain scans in medical imaging applications. \n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
